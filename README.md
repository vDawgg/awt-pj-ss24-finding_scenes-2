# awt-pj-ss24-finding_scenes-2


## Project Goal

This project enables the automatic extraction of semantic metadata from YouTube videos using their links. The extracted metadata is provided as a JSON object.

## Pipeline

The pipeline has following structure

![alt text](./utils/assets/Pipeline.png)


First, the user provides a YouTube link, which is received by FastAPI. FastAPI forwards this link to both PyTube and YT-DLP to obtain the video and essential data from YouTube, including the audio transcript. Then, PyScence Detect is employed to segment the video into meaningful scenes. Katna is utilized to extract the most significant keyframes that represent each scene. Using these keyframes and the audio transcript, a Vision-Language Model (VLM) extracts important metadata from the scenes. Since the metadata generated by the VLM may lack context for the entire scene or video, a Large Language Model (LLM) is then used to contextualize this data. Finally, the contextualized data from the LLM is combined with the metadata already provided by PyTube to form a metadata object

## Metadata object json

```json
{
  "MetaDataObject": {
    "youtube_title": "str",
    "youtube_description": "str",
    "published_date": "str",
    "youtube_video_id": "str",
    "youtube_thumbnail_url": "str",
    "youtube_rating": "str",
    "youtube_views": "str",
    "youtube_age_restricted": "str",
    "youtube_keywords": ["str"],
    "youtube_author": "str",
    "youtube_channel_id": "str",
    "youtube_length": "int",
    "url": "str",
    "llm_description": "str",
    "learning_resource_type": "str",
    "intended_end_user_role": "str",
    "context": "str",
    "difficulty_level": "str",
    "discipline": "str",
    "educational_level": "str",
    "target_audience_age": "str",
    "typical_learning_time": "str",
    "scene_objects": ["SceneObject"]
  }

   "SceneObject":{
    "duration": "str",
    "scene_start": "str",
    "scene_end": "str",
    "title": "str",
    "caption": "str",
    "key-concepts": "str",
    "questions": "str",
    "text": "str",
    "resources": "str",
    "language": "str",
    "video_type": "str"
  }
}
```
## Setup


To install the project's python libraries, run the following command in your terminal:

```bash
pip install -r requirements.txt
```

Additionally, to install flash attention run:

```bash
pip install flash-attn --no-build-isolation
```

As with most python projects we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).

### Dependencies

To install the non python dependencies of the project run the following, depending on your systen:

#### Arch

`sudo pacman -S ffmpeg`

#### Ubuntu

`sudo apt-get install ffmpeg`

### Models

The models are downloaded from [huggingface](https://huggingface.co/) when running the program for the first time. To download Mistral-7B-Instruct-v0.3 you need to first create a HF account and ask for access on the [model page](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3). After access has been granted, create an access token with write permissions and place it into the [constants file](./utils/constants.py). Now all models shouldd be downloaded without any problems. 

## Running

We offer multiple ways to run the pipeline depending on what interaction level might be needed.
### Colab

If no GPUs are available to you we offer a notebook, which you can use to run the pipeline step by step in [google colab](https://colab.research.google.com/github/vDawgg/awt-pj-ss24-finding_scenes-2/blob/main/colab_pipeline.ipynb) and get a better understanding of the different steps needed to arrive at the metadata for a video.
### Local as script
We also offer a [script](./main.py) which you can run locally. To do this, run ```python main.py <YOUR_YOUTUBE_LINK>```.

### Local with FastApi
Lastly for the least interaction with the actual code you can start up the demo server by running ```python server.py``` and access the front-end under [localhost:8000](http://localhost:8000).
